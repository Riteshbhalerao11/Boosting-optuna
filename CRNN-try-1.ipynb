{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Reshape, Bidirectional, LSTM, Dense, Lambda, Rescaling, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import ctc_batch_cost, ctc_decode\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed75966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "image_height, image_width = 50, 200\n",
    "batch_size = 128\n",
    "\n",
    "# Directory containing captcha images -- modify as per your directory name\n",
    "image_dir = 'data'\n",
    "\n",
    "# Create a list of image file paths and corresponding labels\n",
    "image_paths = [str(image) for image in sorted(Path(image_dir).glob(\"*.jpg\"))]\n",
    "labels = [image.stem for image in sorted(Path(image_dir).glob(\"*.jpg\"))]\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "max_length = max([len(label) for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all unique characters in the labels\n",
    "all_possible_characters = sorted(set(\"\".join(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_possible_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of characters to integers and integers to characters\n",
    "char_to_int = {char: i for i, char in enumerate(all_possible_characters)}\n",
    "int_to_char = {i: char for char, i in char_to_int.items()}\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1)  # Grayscale image\n",
    "    image = tf.image.resize(image, (image_height, image_width))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images and labels\n",
    "images = [preprocess_image(image_path) for image_path in image_paths]\n",
    "encoded_labels = [[char_to_int[char] for char in label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, encoded_labels))\n",
    "\n",
    "# shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sizes for training, validation, and test sets\n",
    "train_size = int(0.8 * len(image_paths))\n",
    "val_size = int(0.1 * len(image_paths))\n",
    "test_size = len(image_paths) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset = dataset.take(train_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "remaining_dataset = dataset.skip(train_size)\n",
    "validation_dataset = remaining_dataset.take(val_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = remaining_dataset.skip(val_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples(dataset, int_to_char, num_samples=5):\n",
    "\n",
    "    # Create an iterator for the dataset\n",
    "    dataset_iter = iter(dataset)\n",
    "\n",
    "    # Iterate through the random samples and visualize them\n",
    "    for i in range(num_samples):\n",
    "        image, label = next(dataset_iter)\n",
    "        # Decode the label (convert integers to characters)\n",
    "        label = [int_to_char[int(x)] for x in label[0].numpy()]\n",
    "\n",
    "        # Display the image and label\n",
    "        plt.figure(figsize=(4, 2))\n",
    "        plt.imshow(image[0, :, :, 0], cmap='gray')\n",
    "        plt.title(\"Label: \" + ''.join(label))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_random_samples(validation_dataset, int_to_char, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-3\n",
    "final_learning_rate = 1e-4\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/100)\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=initial_learning_rate,\n",
    "                decay_steps=steps_per_epoch,\n",
    "                decay_rate=learning_rate_decay_factor,\n",
    "                staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_data = Input(shape=(image_height, image_width, 1), name='input_image')\n",
    "\n",
    "# Standardize values to be in the [0, 1] range\n",
    "x = Rescaling(1./255)(input_data)\n",
    "\n",
    "# Transpose the tensor to shape (None, image_width, image_height, 1)\n",
    "x = Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1, 3]), name=\"transpose\")(x)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=tf.keras.initializers.he_normal(), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=tf.keras.initializers.he_normal(), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer=tf.keras.initializers.he_normal(), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 1), name=\"pool3\")(x) # Pooling over time dimension\n",
    "\n",
    "x = Reshape(target_shape=(image_width // 8, (image_height // 4) * 256), name=\"reshape\")(x)\n",
    "x = Dense(128, activation=\"relu\", kernel_initializer=tf.keras.initializers.he_normal())(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "# Recurrent layers (Bidirectional LSTM)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "# Output layer (CTC)\n",
    "output = Dense(len(all_possible_characters) + 1, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_data, outputs=output, name=\"Captcha-CRNN-model\")\n",
    "\n",
    "# Compile the model with CTC loss\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_length = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')\n",
    "\n",
    "    label_length = label_length * tf.ones(shape=(batch_length, 1), dtype='int64')\n",
    "    input_length = input_length * tf.ones(shape=(batch_length, 1), dtype='int64')\n",
    "    loss = ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=ctc_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell for loading pretrained model of 100 epochs for inference if required and you can skip training.\n",
    "\n",
    "# model.load_weights('model_crnn_100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6348762",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "checkpoint_filepath = 'model_crnn.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True, \n",
    "    save_freq=int(steps_per_epoch*10),       # save model for every 10 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "\n",
    "\n",
    "# Plot training history without accuracy\n",
    "def plot_training_history(history, best_epoch):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the lowest validation loss and the epoch at which it occurred\n",
    "    min_val_loss = min(history.history['val_loss'])\n",
    "    plt.annotate(\n",
    "        f'Lowest Validation Loss: {min_val_loss:.4f}\\nEpoch: {best_epoch + 1}',\n",
    "        xy=(best_epoch, min_val_loss),\n",
    "        xytext=(best_epoch - 3, min_val_loss + 0.1),  # Adjust text position\n",
    "        arrowprops=dict(facecolor='black', arrowstyle='->')\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_visualize_samples(model, dataset, int_to_char, num_samples=5):\n",
    "\n",
    "    # Create an iterator for the dataset\n",
    "    dataset_iter = iter(dataset)\n",
    "\n",
    "    # Create a subplot grid\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(4, 2 * num_samples))\n",
    "\n",
    "    # Iterate through the random samples, decode, and visualize them\n",
    "    for i in range(num_samples):\n",
    "        image, label = next(dataset_iter)\n",
    "\n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(image)\n",
    "        # Decode the predictions using CTC decode\n",
    "        decoded, _ = ctc_decode(predictions, input_length=tf.fill((batch_size,), 25), greedy=True)\n",
    "\n",
    "        # Convert decoded labels to characters\n",
    "        decoded_labels = [int_to_char[int(x)] for x in decoded[0][0,:max_length].numpy()]\n",
    "\n",
    "        # Display the image and decoded label\n",
    "        axes[i].imshow(image[0, :, :, 0], cmap='gray')\n",
    "        axes[i].set_title(\"Decoded: \" + ''.join(decoded_labels))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Adjust spacing and display the grid\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "decode_and_visualize_samples(model, test_dataset, int_to_char, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b03422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataset, int_to_char,size,verbosity=0):\n",
    "     # Create an iterator for the dataset\n",
    "    dataset_iter = iter(dataset)\n",
    "    length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    right = 0\n",
    "    # Iterate through the random samples, decode, and visualize them\n",
    "    for i in range(length):\n",
    "        image, label = next(dataset_iter)\n",
    "        \n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(image,verbose = verbosity)\n",
    "        # Decode the predictions using CTC decode\n",
    "        decoded, _ = ctc_decode(predictions, input_length=tf.fill((label.shape[0],), 25), greedy=True)\n",
    "        # Convert decoded labels to characters\n",
    "        for i in range(label.shape[0]):\n",
    "            true_labels = [int_to_char[int(x)] for x in label[i,:max_length].numpy()]\n",
    "            decoded_labels = [int_to_char[int(x)] if int(x) in int_to_char else '<UNKNOWN>' for x in decoded[0][i,:max_length].numpy()]\n",
    "            if(true_labels == decoded_labels):\n",
    "                right+=1\n",
    "    return right / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might take some time -- change verbosity if you wish to see progress\n",
    "\n",
    "test_accuracy = calculate_accuracy(model,test_dataset,int_to_char,verbosity=0,size = test_size)\n",
    "train_accuracy = calculate_accuracy(model,train_dataset,int_to_char,verbosity=0,size = train_size)\n",
    "validation_accuracy = calculate_accuracy(model,validation_dataset,int_to_char,verbosity=0, size = val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3aa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train accuracy : {train_accuracy}')\n",
    "print(f'Test accuracy : {test_accuracy}')\n",
    "print(f'Validation accuracy : {validation_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
